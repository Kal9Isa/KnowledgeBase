Part 1: Orientation and Setup
    * Set up your Docker environment 
    * Build an image and run it as one container
    * Scale your app to run multiple containers
    * Distribute your app across a cluster
    * Stack services by adding a backend database
    * Deploy your app to production
    
    Docker Concepts
        Docker is a platform for developers and sysadmins to develop, deploy, 
        and run applications with containers. The use of Linux containers to 
        deploy applications is called containerization. Containers are not new, 
        but their use for easily deploying applications is.
        
        Containerization is increasingly popular because containers are:

            * Flexible: Even the most complex applications can be containerized.
            * Lightweight: Containers leverage and share the host kernel.
            * Interchangeable: You can deploy updates and upgrades on-the-fly.
            * Portable: You can build locally, deploy to the cloud, and run anywhere.
            * Scalable: You can increase and automatically distribute container replicas.
            * Stackable: You can stack services vertically and on-the-fly.
            
    Images and Containers
        A "container" is launched by running an image. An "image" is an executable 
        package that includes everything needed to run an application–the code, 
        a runtime, libraries, environment variables, and configuration files.
        
        A "container" is a runtime instance of an image–what the image becomes in 
        memory when executed (that is, an image with state, or a user process). 
        You can see a list of your running containers with the command, '$docker ps', 
        just as you would in Linux.
        
        A container runs natively on Linux and shares the kernel of the host machine 
        with other containers. It runs a discrete process, taking no more memory than 
        any other executable, making it lightweight.
        
        By contrast, a "virtual machine (VM)" runs a full-blown “guest” operating system 
        with virtual access to host resources through a hypervisor. In general, VMs 
        provide an environment with more resources than most applications need.
        
    Prepare Your Docker Container
        Install a maintained version of Docker Community Edition (CE) or Enterprise 
        Edition (EE) on a supported platform.
               
    Recap and Cheat Sheet
        
        ## List Docker CLI commands
        docker
        docker container --help

        ## Display Docker version and info
        docker --version
        docker version
        docker info

        ## Execute Docker image
        docker run image

        ## List Docker images
        docker image ls

        ## List Docker containers (running, all, all in quiet mode)
        docker container ls
        docker container ls --all
        docker container ls -aq
        
    Conclusion of Part One
        Containerization makes Continuous Integration/Continuous Development 
        (CI/CD) seamless. For example:
            * applications have no system dependencies
            * updates can be pushed to any part of a distributed application
            * resource density can be optimized.
       
       With Docker, scaling your application is a matter of spinning up new executables, 
       not running heavy VM hosts.
       
     
Part 2: Containers
    Introduction
        It’s time to begin building an app the Docker way. We start at the bottom of 
        the hierarchy of such an app, which is a container, which we cover in this part. 
        Above this level is a service, which defines how containers behave in production, 
        covered in Part 3. Finally, at the top level is the stack, defining the interactions 
        of all the services, covered in Part 5.

            * Stack
            * Services
            * Container (you are here)
    
    Your New Development Enviornment
        In the past, if you were to start writing a Python app, your first order of business 
        was to install a Python runtime onto your machine. But, that creates a situation where 
        the environment on your machine needs to be perfect for your app to run as expected, 
        and also needs to match your production environment.

        With Docker, you can just grab a portable Python runtime as an image, no installation 
        necessary. Then, your build can include the base Python image right alongside your app 
        code, ensuring that your app, its dependencies, and the runtime, all travel together.

        These portable images are defined by something called a "Dockerfile".   

    Define a Container With Dockerfile
        Dockerfile defines what goes on in the environment inside your container. Access to 
        resources like networking interfaces and disk drives is virtualized inside this environment, 
        which is isolated from the rest of your system, so you need to map ports to the outside world, 
        and be specific about what files you want to “copy in” to that environment. However, after doing 
        that, you can expect that the build of your app defined in this Dockerfile behaves exactly the 
        same wherever it runs.

            
        - Are you behind a proxy server?
            Proxy servers can block connections to your web app once it’s up and running. If you are 
            behind a proxy server, add the following lines to your Dockerfile, using the `ENV` command 
            to specify the host and port for your proxy servers:
            
                # Set proxy server, replace host:port with values for your servers
                ENV http_proxy host:port
                ENV https_proxy host:port

            Add these lines before the call to `pip` so that installation succeeds.
   
        
        - Accessing the name of the host when inside a container retrieves the container ID, which 
        is like the process ID for a running executable.

        - If you are using Docker Toolbox on Windows 7, use the Docker Machine IP instead of localhost. 
        For example, `http://192.168.99.100:4000/`. To find the IP address, use the command `docker-machine ip`.
        
        You can also use the `curl` command in a shell to view the same content.

        - On Windows, explicitly stop the container
            On Windows systems, `CTRL+C` does not stop the container. So, first type `CTRL+C` to get the 
            prompt back (or open another shell), then type `docker container ls` to list the running containers, 
            followed by `docker container stop <Container NAME or ID>` to stop the container. Otherwise, you 
            get an error response from the daemon when you try to re-run the container in the next step.
            
    Share Your Image
        To demonstrate the portability of what we just created, let’s upload our built image and run it 
        somewhere else. After all, you need to know how to push to registries when you want to deploy 
        containers to production.
        
        A "registry" is a collection of repositories, and a "repository" is a collection of images—sort 
        of like a GitHub repository, except the code is already built. An account on a registry can create 
        many repositories. The docker CLI uses Docker’s public registry by default.
        
        - We use Docker’s public registry here just because it’s free and pre-configured, but there are 
        many public ones to choose from, and you can even set up your own private registry using Docker 
        Trusted Registry.
        
    Log In With Your Docker ID
        If you don’t have a Docker account, sign up for one at cloud.docker.com. Make note of your username.

        Log in to the Docker public registry on your local machine.
        
            $docker login
            
        Tag the Image
            The notation for associating a local image with a repository on a registry is `username/repository:tag`. 
            The tag is optional, but recommended, since it is the mechanism that registries use to give 
            Docker images a version. Give the repository and tag meaningful names for the context, such 
            as `get-started:part2`. This puts the image in the `get-started` repository and tag it as `part2`.

            Now, put it all together to tag the image. Run `docker tag image` with your username, repository, 
            and tag names so that the image uploads to your desired destination. The syntax of the command is:
            
                $docker tag image username/repository:tag       

        Publish the Image
            Upload your tagged image to the repository:
            
                $docker push username/repository:tag
                
            Once complete, the results of this upload are publicly available. If you log in to Docker Hub, 
            you see the new image there, with its pull command.
            
        Pull and Run the Image From the Remote Repository                         
            If the image isn’t available locally on the machine, Docker pulls it from the repository.
            
            No matter where `docker run` executes, it pulls your image, along with Python and all 
            the dependencies from requirements.txt, and runs your code. It all travels together in 
            a neat little package, and you don’t need to install anything on the host machine for 
            Docker to run it.
            
        Conclusion of Part Two
            That’s all for this part. In the next section, we learn how to scale our application by 
            running this container in a service.
            
        Recap and Cheat Sheet
            Here is a list of the basic Docker commands from this part, and some related ones if 
            you’d like to explore a bit before moving on.
            
                docker build -t friendlyhello .  # Create image using this directory's Dockerfile
                docker run -p 4000:80 friendlyhello  # Run "friendlyname" mapping port 4000 to 80
                docker run -d -p 4000:80 friendlyhello         # Same thing, but in detached mode
                docker container ls                                # List all running containers
                docker container ls -a             # List all containers, even those not running
                docker container stop <hash>           # Gracefully stop the specified container
                docker container kill <hash>         # Force shutdown of the specified container
                docker container rm <hash>        # Remove specified container from this machine
                docker container rm $(docker container ls -a -q)         # Remove all containers
                docker image ls -a                             # List all images on this machine
                docker image rm <image id>            # Remove specified image from this machine
                docker image rm $(docker image ls -a -q)   # Remove all images from this machine
                docker login             # Log in this CLI session using your Docker credentials
                docker tag <image> username/repository:tag  # Tag <image> for upload to registry
                docker push username/repository:tag            # Upload tagged image to registry
                docker run username/repository:tag                   # Run image from a registry
                
Part 3: Services

    Prerequisite
        Get Docker Compose. On Docker for Mac and Docker for Windows it’s pre-installed, so you’re 
        good-to-go. On Linux systems you need to install it directly. On pre Windows 10 systems 
        without Hyper-V, use Docker Toolbox.
        
        Compose is a tool for defining and running multi-container Docker applications. With Compose, 
        you use a YAML file to configure your application’s services. Then, with a single command, 
        you create and start all the services from your configuration.

        Compose works in all environments: production, staging, development, testing, as well as CI 
        workflows. You can learn more about each case in Common Use Cases.

        Using Compose is basically a three-step process:

            * Define your app’s environment with a Dockerfile so it can be reproduced anywhere.

            * Define the services that make up your app in docker-compose.yml so they can be run 
            together in an isolated environment.

            * Run docker-compose up and Compose starts and runs your entire app.
        
        Common Use Cases:
            * Development enviornments
                When you’re developing software, the ability to run an application in an isolated 
                environment and interact with it is crucial. The Compose command line tool can be 
                used to create the environment and interact with it.

                The Compose file provides a way to document and configure all of the application’s 
                service dependencies (databases, queues, caches, web service APIs, etc). Using the 
                Compose command line tool you can create and start one or more containers for each 
                dependency with a single command (`docker-compose up`).

            * Automated testing enviornments
                An important part of any Continuous Deployment or Continuous Integration process is 
                the automated test suite. Automated end-to-end testing requires an environment in which 
                to run tests. Compose provides a convenient way to create and destroy isolated testing 
                environments for your test suite. By defining the full environment in a Compose file, 
                you can create and destroy these environments in just a few commands:

                    $ docker-compose up -d
                    $ ./run_tests
                    $ docker-compose down
            
            * Single host deployment
                You can use Compose to deploy to a "remote Docker Engine". The Docker Engine may be a 
                single instance provisioned with Docker Machine or an entire "Docker Swarm" cluster.   
                
    Introduction
        In this part we scale our application and enable load-balancing.
        
    About Services
        In a distributed application, different pieces of the app are called “services.”

        Services are really just “containers in production.” A service only runs one image, but it codifies 
        the way that image runs—what ports it should use, how many replicas of the container should run so 
        the service has the capacity it needs, and so on. Scaling a service changes the number of container 
        instances running that piece of software, assigning more computing resources to the service in the 
        process.

        Luckily it’s very easy to define, run, and scale services with the Docker platform – just write a 
        `docker-compose.yml` file.
    
    Your first `docker-compose.yml` file
        a `docker-compose.yml` file is a YAML file that defines how Docker containers 
        should behave in production.

    Run Your New Load-Balanced App
        A single container running in a service is called a task. Tasks are given unique IDs that numerically 
        increment, up to the number of replicas you defined in docker-compose.yml. 
        
        - Windows 10 PowerShell should already have `curl` available, if not grab a terminal emulator or 
        download wget for windows
        
    Recap and Cheat Sheet
        To recap, while typing `docker run` is simple enough, the true implementation of a container in 
        production is running it as a service. Services codify a container’s behavior in a Compose file, 
        and this file can be used to scale, limit, and redeploy our app. Changes to the service can be 
        applied in place, as it runs, using the same command that launched the service: `docker stack deploy`.
        
            docker stack ls                                            # List stacks or apps
            docker stack deploy -c <composefile> <appname>  # Run the specified Compose file
            docker service ls                 # List running services associated with an app
            docker service ps <service>                  # List tasks associated with an app
            docker inspect <task or container>                   # Inspect task or container
            docker container ls -q                                      # List container IDs
            docker stack rm <appname>                             # Tear down an application
            docker swarm leave --force      # Take down a single node swarm from the manager
            
Part 4: Swarms
    Introduction
        Here in part 4, you deploy this application onto a cluster, running it on multiple machines. 
        Multi-container, multi-machine applications are made possible by joining multiple machines 
        into a “Dockerized” cluster called a swarm.

    Understanding Swarm Clusters
        A "swarm" is a group of machines that are running Docker and joined into a cluster. After that 
        has happened, you continue to run the Docker commands you’re used to, but now they are executed 
        on a cluster by a swarm manager. The machines in a swarm can be physical or virtual. After joining 
        a swarm, they are referred to as "nodes".

        Swarm managers can use several strategies to run containers, such as “emptiest node” – which fills 
        the least utilized machines with containers. Or “global”, which ensures that each machine gets exactly 
        one instance of the specified container. You instruct the swarm manager to use these strategies in the 
        Compose file, just like the one you have already been using.

        Swarm managers are the only machines in a swarm that can execute your commands, or authorize other 
        machines to join the swarm as "workers". Workers are just there to provide capacity and do not have 
        the authority to tell any other machine what it can and cannot do.

        Up until now, you have been using Docker in a single-host mode on your local machine. But Docker also 
        can be switched into swarm mode, and that’s what enables the use of swarms. Enabling swarm mode instantly 
        makes the current machine a swarm manager. From then on, Docker runs the commands you execute on the 
        swarm you’re managing, rather than just on the current machine.
        
